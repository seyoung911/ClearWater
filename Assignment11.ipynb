{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbAg7+zK3o3l3PlEJ0G8ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyoung911/ClearWater/blob/master/Assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV_B1dl7XX8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "98ba5422-91b0-4e2a-c049-2df7b17531b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIkMVsVZU7Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "review_data = load_files(\"/content/drive/My Drive/Colab Notebooks/movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', ' ', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5CfSw7Qjr-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def sigmoid(x, Derivative = False):\n",
        "  if not Derivative:\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  else:\n",
        "    out = sigmoid(x)\n",
        "    return out * (1 - out)\n",
        "\n",
        "class backPropNN:\n",
        "\n",
        "  # class Members( iternal variables that are accessed with backPropNN.member)\n",
        "\n",
        "  numLayers = 0\n",
        "  shape = None\n",
        "  weights = []\n",
        "\n",
        "  def __init__(self, numNodes):\n",
        "\n",
        "    \"\"\"Initialize the NN - setup the layers and initial weights\"\"\"\n",
        "    self.numLayers = len(numNodes) - 1\n",
        "    self.shape = numNodes\n",
        "\n",
        "    for (i, j) in zip(numNodes[:-1], numNodes[1:]):\n",
        "      self.weights.append(np.random.normal( 0.1, (j, i+1)))\n",
        "\n",
        "    self._layerInput = []\n",
        "    self._layerOutput = []\n",
        "\n",
        "  def FP(self, input):\n",
        "    \"\"\"Get the input data and run it through the NN\"\"\"\n",
        "\n",
        "    self._layerInput = []\n",
        "    self._layerOutput = []\n",
        "\n",
        "    numExamples = input.shape[0] #input 행렬의 행 갯수. = 예시 데이터 갯수(len(X))\n",
        "\n",
        "    for index in range(self.numLayers):\n",
        "      if (index == 0) :\n",
        "        layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, numExamples])]))\n",
        "      else :\n",
        "        layerInput = self.weights[index].dot(np.vstack([self._layerOutput[-1], np.ones([1, numExamples])]))\n",
        "        \n",
        "      self._layerInput.append(layerInput)\n",
        "      self._layerOutput.append(sigmoid(layerInput))\n",
        "\n",
        "    return self._layerOutput[-1].T\n",
        "\n",
        "  def backProp(self, input, target, trainingRate = 0.2):\n",
        "\n",
        "    \"\"\"Get the error, deltas and back propagate to update the weights\"\"\"\n",
        "\n",
        "    delta = []\n",
        "    numExamples = input.shape[0]\n",
        "\n",
        "    self.FP(input)\n",
        "    \n",
        "    for index in reversed(range(self.numLayers)):\n",
        "\n",
        "      if index == self.numLayers -1:\n",
        "        output_delta = self._layerOutput[index] - target.T\n",
        "        error = np.sum(ouput_delta ** 2)\n",
        "        delta.append(output_delta * sigmoid(self._layerInput[index],True))\n",
        "\n",
        "      else:\n",
        "        delta_pullback = self.weights[index +1].T.dot(delta[-1])\n",
        "        delta.append(delta_pullback[:-1, :] * sigmoid(self._layerInput[index],True))\n",
        "  \n",
        "    for index in range(self.numLayers):\n",
        "      delta_index = self.numLayers -1-index\n",
        "\n",
        "      if index == 0:\n",
        "        layerOutput = np.vstack([input.T, np.ones([1, numExamples])])\n",
        "      else:\n",
        "        layerOutput = np.vstack([self._layerOutput[index - 1], np.ones([1,self._layerOutput[index -1].shape[1]])])\n",
        "       \n",
        "        return error\n",
        "\n",
        "\n",
        "NN = backPropNN((1400,50,1))\n",
        "Error = NN.backProp(X_train, y_train)\n",
        "Output = NN.FP(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec155mFgVWxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "dd307a83-a00e-4795-8f59-5b9c86dbb050"
      },
      "source": [
        "nn_architecture = [\n",
        "    {\"input_dim\": 1500, \"output_dim\": 700, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 700, \"output_dim\": 300, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 300, \"output_dim\": 100, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 100, \"output_dim\": 20, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 20, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
        "]\n",
        "\n",
        "def init_layers(nn_architecture, seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    params_values = {}\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, layer_input_size) * 0.1\n",
        "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, 1) * 0.1\n",
        "        \n",
        "    return params_values\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;\n",
        "\n",
        "\n",
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "\n",
        "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "    \n",
        "    if activation is \"relu\":\n",
        "        activation_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activation_func = sigmoid\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    return activation_func(Z_curr), Z_curr\n",
        "\n",
        "def full_forward_propagation(X, params_values, nn_architecture):\n",
        "    memory = {}\n",
        "    A_curr = X\n",
        "    \n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "        \n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "       \n",
        "    return A_curr, memory\n",
        "\n",
        "def get_cost_value(Y_hat, Y):\n",
        "    m = Y_hat.shape[1]\n",
        "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()\n",
        "\n",
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    if activation is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "    \n",
        "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "    return dA_prev, dW_curr, db_curr\n",
        "\n",
        "\n",
        "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
        "    grads_values = {}\n",
        "    m = Y.shape[1]\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "   \n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads_values\n",
        "\n",
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "    for layer_idx, layer in enumerate(nn_architecture):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return params_values;\n",
        "\n",
        "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
        "    params_values = init_layers(nn_architecture, 2)\n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
        "        cost = get_cost_value(Y_hat, Y)\n",
        "        cost_history.append(cost)\n",
        "        accuracy = get_accuracy_value(Y_hat, Y)\n",
        "        accuracy_history.append(accuracy)\n",
        "        \n",
        "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
        "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
        "        \n",
        "    return params_values, cost_history, accuracy_history\n",
        "\n",
        "train_matrix = train(X_train,y_train,nn_architecture, 2000, 0.1)\n",
        "test_matrix = train(X_test,y_test,nn_architecture, 3000, 0.1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-1487c2e21cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mtest_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-132-1487c2e21cea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cost_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-132-1487c2e21cea>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mW_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mA_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_function_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-132-1487c2e21cea>\u001b[0m in \u001b[0;36msingle_layer_forward_propagation\u001b[0;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (700,1500) and (1400,1500) not aligned: 1500 (dim 1) != 1400 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtT7hPU5LUdt",
        "colab_type": "text"
      },
      "source": [
        "#1. Plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEAir12MLe78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([i for i in range(len(train_matrix.cost_history))],(train_matrix.cost_history),color='blue')\n",
        "plt.plot([i for i in range(len(test_matrix.cost_history))],(test_matrix.cost_history),color= 'red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9iTS3F2LcBP",
        "colab_type": "text"
      },
      "source": [
        "#2. Plot the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFyvrZWJLfeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([i for i in range(len(train_matrix.cost_history))],(train_matrix.accuracy_history),color='blue')\n",
        "plt.plot([i for i in range(len(test_matrix.cost_history))],(test_matrix.accuracy_history),color= 'red')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}